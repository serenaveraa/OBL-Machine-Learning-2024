{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RED NEURONAL PARA PREDECIR GENERO BASADO EN LA DESCRIPCION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5554, 1132, 4, 2, 5555, 9, 532, 11, 2, 1037, 3001, 21, 253, 2, 332, 260, 41, 46, 4, 2, 3847, 190, 3848, 8, 8, 8, 75, 149, 4, 51, 4, 174, 47, 21, 25, 33, 5556, 54, 193, 37, 270, 49, 2, 533, 5557, 5558, 5559, 2, 950, 1796, 3002, 5560, 2, 375, 199, 3849, 5561, 3, 5562, 63, 25, 175, 43, 5, 3003, 3, 75, 6, 2, 1797, 8, 8, 709, 14, 25, 55, 388, 1038, 2078, 13, 277, 14, 951, 11, 2, 47, 14, 638, 8, 8, 8, 28, 1564, 9, 2, 504, 476, 141, 10, 5563, 12, 5, 3850, 408, 4, 2, 1565, 3004, 357, 16, 5564, 5565, 4, 2, 465, 5566, 4, 2, 3004, 357, 23, 5567, 6, 3005, 2, 3004, 3006, 16, 1798], [1133, 23, 1378, 19, 271, 3, 2, 84, 882, 9, 19, 639, 4, 2079, 883, 11, 17, 389, 5568, 1566, 4, 952, 5569, 2079, 76, 5570, 3, 5571, 2466, 234, 466, 6, 1134, 317, 3, 3007, 884, 6, 429, 3008, 4, 1039, 4, 1133, 231, 7, 35, 953], [430, 37, 5572, 9, 2, 127, 3, 118, 376, 7, 2, 430, 351, 30, 26, 9, 5, 38, 600, 333, 1040, 318, 557, 11, 388, 1038, 21, 1567, 24, 449, 3, 225, 1799], [291, 254, 2, 39, 4, 390, 9, 2, 261, 1568, 226, 7, 2, 246, 4, 291, 30, 3, 2, 682, 115, 6, 2, 42, 10, 76, 66, 954, 26, 56, 710, 21, 69, 33, 2080, 7, 2, 1800, 3, 5573, 16, 17, 1135, 21, 27, 1801, 1802, 358, 2, 10, 955, 128, 359, 1041, 49, 601, 1136, 6, 2, 162, 51, 5574, 3851, 2, 2467, 3, 2, 79, 1379, 137, 2, 22, 505, 1567, 1137, 11, 72, 21, 75, 177, 13, 5, 205, 409, 5575, 808, 135, 2, 292, 116, 477, 16, 40, 2, 30, 1246, 194], [3852, 3853, 9, 5, 3009, 476, 3854, 10, 255, 18, 5576, 5, 232, 1042, 10, 3010, 2, 10, 57, 253, 199, 3855, 2468, 2, 10, 9, 3856, 12, 51, 4, 2, 88, 80, 5577, 4, 3855, 2468, 3, 2, 711, 4, 5, 640, 15, 467, 4, 46, 8, 8, 8, 3852, 3853, 683, 431, 3857, 7, 232, 3, 2, 10, 278, 5, 213, 506, 468, 5578, 12, 282, 885, 2, 408, 9, 6, 886, 70, 3, 1380, 32, 2469, 5579, 8, 8, 8, 2, 10, 3011, 44, 130, 391, 3, 1569, 2, 712, 10, 9, 432, 36, 6, 5580]]\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karina\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 - 7s - 27ms/step - accuracy: 0.1251 - loss: 2.6816 - val_accuracy: 0.1682 - val_loss: 2.4423\n",
      "Epoch 2/10\n",
      "276/276 - 5s - 17ms/step - accuracy: 0.1437 - loss: 2.5026 - val_accuracy: 0.1682 - val_loss: 2.4250\n",
      "Epoch 3/10\n",
      "276/276 - 5s - 18ms/step - accuracy: 0.1490 - loss: 2.4604 - val_accuracy: 0.1673 - val_loss: 2.4176\n",
      "Epoch 4/10\n",
      "276/276 - 5s - 17ms/step - accuracy: 0.1572 - loss: 2.4328 - val_accuracy: 0.1682 - val_loss: 2.4152\n",
      "Epoch 5/10\n",
      "276/276 - 7s - 25ms/step - accuracy: 0.1656 - loss: 2.3991 - val_accuracy: 0.1600 - val_loss: 2.4183\n",
      "Epoch 6/10\n",
      "276/276 - 8s - 30ms/step - accuracy: 0.1760 - loss: 2.3724 - val_accuracy: 0.1636 - val_loss: 2.4440\n",
      "Epoch 7/10\n",
      "276/276 - 9s - 31ms/step - accuracy: 0.1843 - loss: 2.3377 - val_accuracy: 0.1650 - val_loss: 2.4798\n",
      "Epoch 8/10\n",
      "276/276 - 9s - 33ms/step - accuracy: 0.1956 - loss: 2.3149 - val_accuracy: 0.1646 - val_loss: 2.4921\n",
      "Epoch 9/10\n",
      "276/276 - 6s - 20ms/step - accuracy: 0.2032 - loss: 2.2900 - val_accuracy: 0.1659 - val_loss: 2.5539\n",
      "Epoch 10/10\n",
      "276/276 - 5s - 17ms/step - accuracy: 0.2044 - loss: 2.2741 - val_accuracy: 0.1641 - val_loss: 2.6245\n",
      "69/69 - 0s - 6ms/step - accuracy: 0.1641 - loss: 2.6245\n",
      "Validation Accuracy: 16.41%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GlobalMaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Cargar el dataframe\n",
    "data = pd.read_csv('./dataset/train.csv')\n",
    "\n",
    "# 1. Preprocesamiento de texto\n",
    "def convert_to_numeric(value):\n",
    "    if isinstance(value, str):\n",
    "        if 'K' in value:\n",
    "            return float(value.replace('K', '')) * 1_000\n",
    "        elif 'M' in value:\n",
    "            return float(value.replace('M', '')) * 1_000_000\n",
    "    return float(value)\n",
    "\n",
    "def preprocess_data(data):\n",
    "    data[\"Number of Reviews\"] = data['Number of Reviews'].apply(convert_to_numeric)\n",
    "    data[\"Wishlist\"] = data[\"Wishlist\"].apply(convert_to_numeric)\n",
    "\n",
    "    # Asegurar que todos los valores de la columna 'Summary' sean cadenas\n",
    "    data[\"Summary\"] = data[\"Summary\"].fillna(\"\").astype(str)\n",
    "\n",
    "    # Crear y ajustar el tokenizer\n",
    "    tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(data[\"Summary\"])\n",
    "    sequences = tokenizer.texts_to_sequences(data[\"Summary\"])\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    # Verificar las secuencias creadas\n",
    "    print(sequences[:5])\n",
    "    word_index = tokenizer.word_index\n",
    "    max_length = 50  # Longitud máxima de secuencia\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "    # Codificar las etiquetas\n",
    "    label_encoder = LabelEncoder()\n",
    "    data[\"Genre\"] = label_encoder.fit_transform(data[\"Genre\"])\n",
    "    labels = to_categorical(data[\"Genre\"])\n",
    "    return data, padded_sequences, labels, max_length\n",
    "\n",
    "data = preprocess_data(data)\n",
    "\n",
    "# Dividir datos en entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    padded_sequences, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Definir el modelo\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=max_length),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(labels.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# 3. Compilar el modelo\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 4. Entrenar el modelo\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 5. Evaluar el modelo\n",
    "loss, accuracy = model.evaluate(X_val, y_val, verbose=2)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Summary'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m test_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dataset/test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Preprocess the 'Summary' column in the test data\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m test_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummary\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSummary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Convert the test 'Summary' to sequences and pad them\u001b[39;00m\n\u001b[0;32m      8\u001b[0m test_sequences \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(test_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummary\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Summary'"
     ]
    }
   ],
   "source": [
    "# Load the test data\n",
    "test_data = pd.read_csv('./dataset/test.csv')\n",
    "\n",
    "# Preprocess the 'Summary' column in the test data\n",
    "\n",
    "# Convert the test 'Summary' to sequences and pad them\n",
    "test_padded_sequences = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(test_padded_sequences, verbose=1)\n",
    "\n",
    "# Convert predictions to genre labels\n",
    "predicted_genres = label_encoder.inverse_transform(predictions.argmax(axis=1))\n",
    "\n",
    "# Save results to a new CSV file\n",
    "output = pd.DataFrame({\n",
    "    'id': test_data['id'], \n",
    "    'Genre': predicted_genres\n",
    "})\n",
    "output.to_csv('./dataset/predicted_genres.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to './dataset/predicted_genres.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
