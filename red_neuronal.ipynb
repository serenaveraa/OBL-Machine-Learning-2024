{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karina\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 - 12s - 42ms/step - accuracy: 0.1278 - loss: 2.6636 - val_accuracy: 0.1682 - val_loss: 2.4460\n",
      "Epoch 2/20\n",
      "276/276 - 8s - 31ms/step - accuracy: 0.1455 - loss: 2.4996 - val_accuracy: 0.1682 - val_loss: 2.4186\n",
      "Epoch 3/20\n",
      "276/276 - 13s - 48ms/step - accuracy: 0.1599 - loss: 2.4420 - val_accuracy: 0.1700 - val_loss: 2.4079\n",
      "Epoch 4/20\n",
      "276/276 - 14s - 51ms/step - accuracy: 0.1589 - loss: 2.4006 - val_accuracy: 0.1704 - val_loss: 2.4192\n",
      "Epoch 5/20\n",
      "276/276 - 14s - 50ms/step - accuracy: 0.1759 - loss: 2.3581 - val_accuracy: 0.1704 - val_loss: 2.4514\n",
      "Epoch 6/20\n",
      "276/276 - 13s - 47ms/step - accuracy: 0.1863 - loss: 2.3378 - val_accuracy: 0.1632 - val_loss: 2.4523\n",
      "Epoch 7/20\n",
      "276/276 - 13s - 49ms/step - accuracy: 0.1898 - loss: 2.3168 - val_accuracy: 0.1664 - val_loss: 2.4857\n",
      "Epoch 8/20\n",
      "276/276 - 14s - 49ms/step - accuracy: 0.1963 - loss: 2.2984 - val_accuracy: 0.1655 - val_loss: 2.5898\n",
      "Epoch 9/20\n",
      "276/276 - 14s - 49ms/step - accuracy: 0.2040 - loss: 2.2767 - val_accuracy: 0.1614 - val_loss: 2.6674\n",
      "Epoch 10/20\n",
      "276/276 - 14s - 51ms/step - accuracy: 0.2059 - loss: 2.2737 - val_accuracy: 0.1627 - val_loss: 2.6491\n",
      "Epoch 11/20\n",
      "276/276 - 14s - 49ms/step - accuracy: 0.2102 - loss: 2.2641 - val_accuracy: 0.1641 - val_loss: 2.6519\n",
      "Epoch 12/20\n",
      "276/276 - 14s - 49ms/step - accuracy: 0.2151 - loss: 2.2646 - val_accuracy: 0.1623 - val_loss: 2.6224\n",
      "Epoch 13/20\n",
      "276/276 - 14s - 49ms/step - accuracy: 0.2146 - loss: 2.2486 - val_accuracy: 0.1627 - val_loss: 2.6867\n",
      "Epoch 14/20\n",
      "276/276 - 14s - 50ms/step - accuracy: 0.2172 - loss: 2.2380 - val_accuracy: 0.1618 - val_loss: 2.7521\n",
      "Epoch 15/20\n",
      "276/276 - 14s - 50ms/step - accuracy: 0.2199 - loss: 2.2279 - val_accuracy: 0.1627 - val_loss: 2.7957\n",
      "Epoch 16/20\n",
      "276/276 - 14s - 51ms/step - accuracy: 0.2221 - loss: 2.2239 - val_accuracy: 0.1623 - val_loss: 2.8614\n",
      "Epoch 17/20\n",
      "276/276 - 14s - 51ms/step - accuracy: 0.2264 - loss: 2.2163 - val_accuracy: 0.1627 - val_loss: 2.9494\n",
      "Epoch 18/20\n",
      "276/276 - 14s - 50ms/step - accuracy: 0.2272 - loss: 2.2152 - val_accuracy: 0.1650 - val_loss: 2.8442\n",
      "Epoch 19/20\n",
      "276/276 - 14s - 50ms/step - accuracy: 0.2273 - loss: 2.2140 - val_accuracy: 0.1659 - val_loss: 2.8718\n",
      "Epoch 20/20\n",
      "276/276 - 13s - 49ms/step - accuracy: 0.2268 - loss: 2.2112 - val_accuracy: 0.1655 - val_loss: 2.8414\n",
      "69/69 - 1s - 21ms/step - accuracy: 0.1655 - loss: 2.8414\n",
      "Validation Accuracy: 16.55%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GlobalMaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Cargar el dataframe\n",
    "data = pd.read_csv('./dataset/train.csv')\n",
    "\n",
    "# 1. Preprocesamiento de texto\n",
    "def convert_to_numeric(value):\n",
    "    if isinstance(value, str):\n",
    "        if 'K' in value:\n",
    "            return float(value.replace('K', '')) * 1_000\n",
    "        elif 'M' in value:\n",
    "            return float(value.replace('M', '')) * 1_000_000\n",
    "    return float(value)\n",
    "\n",
    "# No es necesario aplicar la conversión a las demás columnas ya que solo usamos 'Summary'\n",
    "data[\"Summary\"] = data[\"Summary\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Crear y ajustar el tokenizer para la columna \"Summary\"\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(data[\"Summary\"])\n",
    "sequences = tokenizer.texts_to_sequences(data[\"Summary\"])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Longitud máxima de secuencia\n",
    "max_length = 50\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Codificar las etiquetas (género)\n",
    "label_encoder = LabelEncoder()\n",
    "data[\"Genre\"] = label_encoder.fit_transform(data[\"Genre\"])\n",
    "labels = to_categorical(data[\"Genre\"])\n",
    "\n",
    "# Dividir datos en entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    padded_sequences, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Definir el modelo\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=max_length),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(labels.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# 3. Compilar el modelo\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 4. Entrenar el modelo\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 5. Evaluar el modelo\n",
    "loss, accuracy = model.evaluate(X_val, y_val, verbose=2)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step\n",
      "Predictions saved to './dataset/predicted_random_sample_genres.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load the test data\n",
    "# Load the training data again\n",
    "train_data = pd.read_csv('./dataset/train.csv')\n",
    "\n",
    "# Randomly select a subset of rows from the training data for prediction (e.g., 100 samples)\n",
    "random_sample = train_data.sample(n=100, random_state=42)\n",
    "\n",
    "# Preprocess the 'Summary' column in the random sample\n",
    "random_sample[\"Summary\"] = random_sample[\"Summary\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Convert the random sample 'Summary' to sequences and pad them\n",
    "sample_sequences = tokenizer.texts_to_sequences(random_sample[\"Summary\"])\n",
    "sample_padded_sequences = pad_sequences(sample_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(sample_padded_sequences, verbose=1)\n",
    "\n",
    "# Convert predictions to genre labels\n",
    "predicted_genres = label_encoder.inverse_transform(predictions.argmax(axis=1))\n",
    "\n",
    "# Save results to a new CSV file\n",
    "output = pd.DataFrame({\n",
    "    'id': random_sample['id'], \n",
    "    'Genre': predicted_genres\n",
    "})\n",
    "output.to_csv('./dataset/predicted_random_sample_genres.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to './dataset/predicted_random_sample_genres.csv'\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
