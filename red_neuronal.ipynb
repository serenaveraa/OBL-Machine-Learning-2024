{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spark/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 - 7s - 26ms/step - accuracy: 0.1394 - loss: 2.6804 - val_accuracy: 0.1682 - val_loss: 2.4314\n",
      "Epoch 2/20\n",
      "276/276 - 7s - 26ms/step - accuracy: 0.1500 - loss: 2.4959 - val_accuracy: 0.1682 - val_loss: 2.4164\n",
      "Epoch 3/20\n",
      "276/276 - 7s - 26ms/step - accuracy: 0.1481 - loss: 2.4554 - val_accuracy: 0.1682 - val_loss: 2.4109\n",
      "Epoch 4/20\n",
      "276/276 - 7s - 26ms/step - accuracy: 0.1618 - loss: 2.4098 - val_accuracy: 0.1673 - val_loss: 2.4196\n",
      "Epoch 5/20\n",
      "276/276 - 8s - 27ms/step - accuracy: 0.1715 - loss: 2.3814 - val_accuracy: 0.1627 - val_loss: 2.4172\n",
      "Epoch 6/20\n",
      "276/276 - 8s - 28ms/step - accuracy: 0.1831 - loss: 2.3441 - val_accuracy: 0.1618 - val_loss: 2.4307\n",
      "Epoch 7/20\n",
      "276/276 - 7s - 27ms/step - accuracy: 0.1945 - loss: 2.3227 - val_accuracy: 0.1636 - val_loss: 2.4797\n",
      "Epoch 8/20\n",
      "276/276 - 7s - 25ms/step - accuracy: 0.1982 - loss: 2.3032 - val_accuracy: 0.1627 - val_loss: 2.5299\n",
      "Epoch 9/20\n",
      "276/276 - 7s - 25ms/step - accuracy: 0.2046 - loss: 2.2861 - val_accuracy: 0.1600 - val_loss: 2.5438\n",
      "Epoch 10/20\n",
      "276/276 - 7s - 25ms/step - accuracy: 0.2082 - loss: 2.2714 - val_accuracy: 0.1582 - val_loss: 2.5753\n",
      "Epoch 11/20\n",
      "276/276 - 7s - 25ms/step - accuracy: 0.2104 - loss: 2.2619 - val_accuracy: 0.1636 - val_loss: 2.5542\n",
      "Epoch 12/20\n",
      "276/276 - 7s - 26ms/step - accuracy: 0.2142 - loss: 2.2502 - val_accuracy: 0.1587 - val_loss: 2.6200\n",
      "Epoch 13/20\n",
      "276/276 - 7s - 25ms/step - accuracy: 0.2179 - loss: 2.2404 - val_accuracy: 0.1609 - val_loss: 2.7337\n",
      "Epoch 14/20\n",
      "276/276 - 7s - 25ms/step - accuracy: 0.2169 - loss: 2.2367 - val_accuracy: 0.1609 - val_loss: 2.7401\n",
      "Epoch 15/20\n",
      "276/276 - 7s - 25ms/step - accuracy: 0.2206 - loss: 2.2317 - val_accuracy: 0.1587 - val_loss: 2.6822\n",
      "Epoch 16/20\n",
      "276/276 - 7s - 25ms/step - accuracy: 0.2244 - loss: 2.2225 - val_accuracy: 0.1600 - val_loss: 2.7570\n",
      "Epoch 17/20\n",
      "276/276 - 7s - 25ms/step - accuracy: 0.2238 - loss: 2.2223 - val_accuracy: 0.1627 - val_loss: 2.8202\n",
      "Epoch 18/20\n",
      "276/276 - 7s - 25ms/step - accuracy: 0.2257 - loss: 2.2106 - val_accuracy: 0.1605 - val_loss: 2.8848\n",
      "Epoch 19/20\n",
      "276/276 - 7s - 25ms/step - accuracy: 0.2276 - loss: 2.2142 - val_accuracy: 0.1587 - val_loss: 2.8549\n",
      "Epoch 20/20\n",
      "276/276 - 7s - 26ms/step - accuracy: 0.2288 - loss: 2.2079 - val_accuracy: 0.1605 - val_loss: 2.8962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 - 0s - 6ms/step - accuracy: 0.1605 - loss: 2.8962\n",
      "Validation Accuracy: 16.05%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GlobalMaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import json\n",
    "from joblib import dump\n",
    "\n",
    "# Cargar el dataframe\n",
    "data = pd.read_csv('./dataset/train.csv')\n",
    "\n",
    "# 1. Preprocesamiento de texto\n",
    "def convert_to_numeric(value):\n",
    "    if isinstance(value, str):\n",
    "        if 'K' in value:\n",
    "            return float(value.replace('K', '')) * 1_000\n",
    "        elif 'M' in value:\n",
    "            return float(value.replace('M', '')) * 1_000_000\n",
    "    return float(value)\n",
    "\n",
    "data[\"Summary\"] = data[\"Summary\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Crear y ajustar el tokenizer para la columna \"Summary\"\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(data[\"Summary\"])\n",
    "sequences = tokenizer.texts_to_sequences(data[\"Summary\"])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Guardar el tokenizer como JSON\n",
    "with open(\"tokenizer.json\", \"w\") as f:\n",
    "    json.dump(tokenizer.to_json(), f)\n",
    "\n",
    "# Longitud máxima de secuencia\n",
    "max_length = 50\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Codificar las etiquetas (género)\n",
    "label_encoder = LabelEncoder()\n",
    "data[\"Genre\"] = label_encoder.fit_transform(data[\"Genre\"])\n",
    "labels = to_categorical(data[\"Genre\"])\n",
    "\n",
    "# Dividir datos en entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    padded_sequences, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Definir el modelo\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=max_length),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(labels.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# 3. Compilar el modelo\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 4. Entrenar el modelo\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 5. Guardar el modelo con joblib\n",
    "model.save(\"game_genre_model.h5\")  # Guardar el modelo en formato HDF5 para TensorFlow\n",
    "dump(label_encoder, \"label_encoder.joblib\")  # Guardar el codificador de etiquetas con joblib\n",
    "\n",
    "# 6. Evaluar el modelo\n",
    "loss, accuracy = model.evaluate(X_val, y_val, verbose=2)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predictions saved to './dataset/predicted_random_sample_genres.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load the test data\n",
    "# Load the training data again\n",
    "train_data = pd.read_csv('./dataset/train.csv')\n",
    "\n",
    "# Randomly select a subset of rows from the training data for prediction (e.g., 100 samples)\n",
    "random_sample = train_data.sample(n=100, random_state=42)\n",
    "\n",
    "# Preprocess the 'Summary' column in the random sample\n",
    "random_sample[\"Summary\"] = random_sample[\"Summary\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Convert the random sample 'Summary' to sequences and pad them\n",
    "sample_sequences = tokenizer.texts_to_sequences(random_sample[\"Summary\"])\n",
    "sample_padded_sequences = pad_sequences(sample_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(sample_padded_sequences, verbose=1)\n",
    "\n",
    "# Convert predictions to genre labels\n",
    "predicted_genres = label_encoder.inverse_transform(predictions.argmax(axis=1))\n",
    "\n",
    "# Save results to a new CSV file\n",
    "output = pd.DataFrame({\n",
    "    'id': random_sample['id'], \n",
    "    'Genre': predicted_genres\n",
    "})\n",
    "output.to_csv('./dataset/predicted_random_sample_genres.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to './dataset/predicted_random_sample_genres.csv'\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
