{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                       Game Title    Year Publisher  North America  \\\n",
      "0   1  Bloodstained: Curse of the Moon  2007.0       NaN           0.08   \n",
      "1   2               Plants vs. Zombies  2007.0       NaN           0.08   \n",
      "2   3   Zero Escape: Zero Time Dilemma  2007.0       NaN           0.08   \n",
      "3   4  Zelda II: The Adventure of Link  1987.0  Nintendo           2.19   \n",
      "4   5                       Yume Nikki  2007.0       NaN           0.08   \n",
      "\n",
      "   Europe  Japan  Rest of World  Global Number of Reviews  \\\n",
      "0    0.02   0.00           0.01    0.17             341.0   \n",
      "1    0.02   0.00           0.01    0.17             719.0   \n",
      "2    0.02   0.00           0.01    0.17             350.0   \n",
      "3    0.50   1.61           0.08    4.38             756.0   \n",
      "4    0.02   0.00           0.01    0.17             770.0   \n",
      "\n",
      "                                             Summary Wishlist Platform  \\\n",
      "0  “Bloodstained: Curse of the Moon” is packed wi...    397.0     2600   \n",
      "1  Zombies are invading your home, and the only d...     82.0     2600   \n",
      "2  Zero Time Dilemma is the third and final entry...    402.0     2600   \n",
      "3  Zelda II: The Adventure of Link is the second ...    290.0      NES   \n",
      "4  Yume Nikki is a 32-Bit freeware game created b...    771.0     2600   \n",
      "\n",
      "       Genre    Rating  \n",
      "0   Platform  7.345609  \n",
      "1     Puzzle  6.511195  \n",
      "2     Puzzle  6.651093  \n",
      "3  Adventure  7.900000  \n",
      "4  Adventure  7.152074  \n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karina\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 - 7s - 25ms/step - accuracy: 0.1364 - loss: 2.6648 - val_accuracy: 0.1682 - val_loss: 2.4373\n",
      "Epoch 2/20\n",
      "276/276 - 5s - 16ms/step - accuracy: 0.1474 - loss: 2.4908 - val_accuracy: 0.1682 - val_loss: 2.4184\n",
      "Epoch 3/20\n",
      "276/276 - 5s - 17ms/step - accuracy: 0.1572 - loss: 2.4393 - val_accuracy: 0.1682 - val_loss: 2.4119\n",
      "Epoch 4/20\n",
      "276/276 - 5s - 17ms/step - accuracy: 0.1550 - loss: 2.4081 - val_accuracy: 0.1623 - val_loss: 2.4222\n",
      "Epoch 5/20\n",
      "276/276 - 5s - 17ms/step - accuracy: 0.1672 - loss: 2.3728 - val_accuracy: 0.1627 - val_loss: 2.4317\n",
      "Epoch 6/20\n",
      "276/276 - 5s - 17ms/step - accuracy: 0.1879 - loss: 2.3320 - val_accuracy: 0.1618 - val_loss: 2.5092\n",
      "Epoch 7/20\n",
      "276/276 - 5s - 18ms/step - accuracy: 0.2002 - loss: 2.3045 - val_accuracy: 0.1609 - val_loss: 2.5441\n",
      "Epoch 8/20\n",
      "276/276 - 5s - 17ms/step - accuracy: 0.2059 - loss: 2.2797 - val_accuracy: 0.1618 - val_loss: 2.5501\n",
      "Epoch 9/20\n",
      "276/276 - 5s - 17ms/step - accuracy: 0.2092 - loss: 2.2641 - val_accuracy: 0.1632 - val_loss: 2.5464\n",
      "Epoch 10/20\n",
      "276/276 - 5s - 18ms/step - accuracy: 0.2162 - loss: 2.2508 - val_accuracy: 0.1655 - val_loss: 2.6593\n",
      "Epoch 11/20\n",
      "276/276 - 5s - 17ms/step - accuracy: 0.2167 - loss: 2.2459 - val_accuracy: 0.1668 - val_loss: 2.6449\n",
      "Epoch 12/20\n",
      "276/276 - 5s - 17ms/step - accuracy: 0.2218 - loss: 2.2359 - val_accuracy: 0.1646 - val_loss: 2.6476\n",
      "Epoch 13/20\n",
      "276/276 - 5s - 17ms/step - accuracy: 0.2250 - loss: 2.2237 - val_accuracy: 0.1609 - val_loss: 2.6675\n",
      "Epoch 14/20\n",
      "276/276 - 5s - 18ms/step - accuracy: 0.2265 - loss: 2.2230 - val_accuracy: 0.1668 - val_loss: 2.7269\n",
      "Epoch 15/20\n",
      "276/276 - 5s - 18ms/step - accuracy: 0.2242 - loss: 2.2194 - val_accuracy: 0.1650 - val_loss: 2.7788\n",
      "Epoch 16/20\n",
      "276/276 - 5s - 18ms/step - accuracy: 0.2299 - loss: 2.2086 - val_accuracy: 0.1632 - val_loss: 2.8498\n",
      "Epoch 17/20\n",
      "276/276 - 5s - 18ms/step - accuracy: 0.2311 - loss: 2.2041 - val_accuracy: 0.1650 - val_loss: 2.7519\n",
      "Epoch 18/20\n",
      "276/276 - 5s - 18ms/step - accuracy: 0.2314 - loss: 2.2042 - val_accuracy: 0.1655 - val_loss: 2.8989\n",
      "Epoch 19/20\n",
      "276/276 - 5s - 17ms/step - accuracy: 0.2320 - loss: 2.1945 - val_accuracy: 0.1641 - val_loss: 2.8541\n",
      "Epoch 20/20\n",
      "276/276 - 5s - 17ms/step - accuracy: 0.2338 - loss: 2.1927 - val_accuracy: 0.1636 - val_loss: 2.9424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 - 0s - 6ms/step - accuracy: 0.1636 - loss: 2.9424\n",
      "Validation Accuracy: 16.36%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GlobalMaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import json\n",
    "from joblib import dump\n",
    "\n",
    "# Cargar el dataframe\n",
    "data = pd.read_csv('./dataset/train.csv')\n",
    "\n",
    "# 1. Preprocesamiento de texto\n",
    "def convert_to_numeric(value):\n",
    "    if isinstance(value, str):\n",
    "        if 'K' in value:\n",
    "            return float(value.replace('K', '')) * 1_000\n",
    "        elif 'M' in value:\n",
    "            return float(value.replace('M', '')) * 1_000_000\n",
    "    return float(value)\n",
    "\n",
    "data[\"Summary\"] = data[\"Summary\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Crear y ajustar el tokenizer para la columna \"Summary\"\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(data[\"Summary\"])\n",
    "sequences = tokenizer.texts_to_sequences(data[\"Summary\"])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Guardar el tokenizer como JSON\n",
    "with open(\"tokenizer.json\", \"w\") as f:\n",
    "    json.dump(tokenizer.to_json(), f)\n",
    "\n",
    "# Longitud máxima de secuencia\n",
    "max_length = 50\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Codificar las etiquetas (género)\n",
    "label_encoder = LabelEncoder()\n",
    "data[\"Genre\"] = label_encoder.fit_transform(data[\"Genre\"])\n",
    "labels = to_categorical(data[\"Genre\"])\n",
    "\n",
    "# Dividir datos en entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    padded_sequences, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Definir el modelo\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=max_length),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(labels.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# 3. Compilar el modelo\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 4. Entrenar el modelo\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 5. Guardar el modelo con joblib\n",
    "model.save(\"game_genre_model.h5\")  # Guardar el modelo en formato HDF5 para TensorFlow\n",
    "dump(label_encoder, \"label_encoder.joblib\")  # Guardar el codificador de etiquetas con joblib\n",
    "\n",
    "# 6. Evaluar el modelo\n",
    "loss, accuracy = model.evaluate(X_val, y_val, verbose=2)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step\n",
      "Predictions saved to './dataset/predicted_random_sample_genres.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load the test data\n",
    "# Load the training data again\n",
    "train_data = pd.read_csv('./dataset/train.csv')\n",
    "\n",
    "# Randomly select a subset of rows from the training data for prediction (e.g., 100 samples)\n",
    "random_sample = train_data.sample(n=100, random_state=42)\n",
    "\n",
    "# Preprocess the 'Summary' column in the random sample\n",
    "random_sample[\"Summary\"] = random_sample[\"Summary\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Convert the random sample 'Summary' to sequences and pad them\n",
    "sample_sequences = tokenizer.texts_to_sequences(random_sample[\"Summary\"])\n",
    "sample_padded_sequences = pad_sequences(sample_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(sample_padded_sequences, verbose=1)\n",
    "\n",
    "# Convert predictions to genre labels\n",
    "predicted_genres = label_encoder.inverse_transform(predictions.argmax(axis=1))\n",
    "\n",
    "# Save results to a new CSV file\n",
    "output = pd.DataFrame({\n",
    "    'id': random_sample['id'], \n",
    "    'Genre': predicted_genres\n",
    "})\n",
    "output.to_csv('./dataset/predicted_random_sample_genres.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to './dataset/predicted_random_sample_genres.csv'\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
